{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSDMobilenetv2 Implementation\n",
    "\n",
    "MobileNet is a class of model that are called for applications in mobile and embedded level devices. Single Shot Detector or SSD is a technique based on a forward convolitional network that generates a collection of fixed size boxes and scores for presence of object class instances in that those boxes.\n",
    "\n",
    "MobileNetSSD is an object detection model that where the SSD layer uses MobileNet as a backbone to achieve fast object detection on low power devices.\n",
    "\n",
    "This notebook will show you how to use the MobileNetSSD model to detect objects in an image using OpenVINOâ„¢ toolkit.\n",
    "Steps included here:\n",
    "\n",
    "1. Downloading and loading the model\n",
    "2. Observing results with original model\n",
    "3. Converting model to IR format\n",
    "4. Observing results with IR model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import openvino.runtime as ov \n",
    "from pathlib import Path\n",
    "from urllib.request import urlopen\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Downloading and loading the model\n",
    "\n",
    "We will be using tensorflow's get_file method to download the model. The model is documented at the [TensorFlowHub](https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1).\n",
    "The specific model we will be using is the one that is trained on the COCO dataset. The model is trained to detect 90 classes of objects. The model is trained to detect objects in images of size 320x320 and also uses a feature pyramid network to detect objects at different scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# here we fetch the model and save it's directory in model_dir,tf.keras.utils.get_file() will download the model if it's not already downloaded and \n",
    "# Path() will convert the string to a path object\n",
    "source = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
    "model_name = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "model_dir = Path(tf.keras.utils.get_file(model_name, source, untar=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : Since Tensorflow Object Detection API has not been installed to keep in line with the requirement.txt, and the model is an Object Detection object, We will be using the serving_default signature to load the model and run inference on that with custom helper functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# here we load the model using tf.saved_model.load() and we get the signature of the model using model.signatures['serving_default']\n",
    "model = tf.saved_model.load(model_dir / 'saved_model')\n",
    "model = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Observing results with original model\n",
    "\n",
    "To keep notebook requirements minimum, we will be using the following helper functions to load the model and run inference on the model, this can also be done with TensorFlow's Object Detection API, but installing and getting that to work is beyond the scope of this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# to make it easier to work with the model we create a function that takes an image and preprocesses it for the model\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Preprocess an image for object detection by the ssd mobilenet model, images are scaled down to 320x320 and color channels are converted to RGB.\n",
    "    Arguments: \n",
    "    image = path to image(string)\"\"\"\n",
    "    image = cv2.imread(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # convert to RGB since model expects it that way\n",
    "    image = cv2.resize(image, (320, 320))\n",
    "    image = tf.convert_to_tensor(image, dtype=tf.uint8)\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# to visualize the output of the model we create a function that takes \n",
    "# the output of the model and the image and displays the image with \n",
    "# bounding boxes of objects detected with a certain confidence threshold\n",
    "def inference_engine(image,output,id_to_display_name,confidence_threshold=0.6):\n",
    "    \"\"\"Visualizes the output of the model. \n",
    "    Arguments:\n",
    "        image = path to image(string)\n",
    "        output = output of the model(dictionary)\n",
    "        id_to_display_name = dictionary mapping class id to class name\n",
    "        confidence_threshold = threshold for displaying bounding boxes(float)\"\"\"\n",
    "    # reading the image and making a copy of it\n",
    "    image = cv2.imread(image)\n",
    "    image_stock = image.copy()\n",
    "    image_stock = cv2.cvtColor(image_stock, cv2.COLOR_BGR2RGB)\n",
    "    # taking relevant outputs from the model\n",
    "    boxes = output['detection_boxes']\n",
    "    classes = output['detection_classes']\n",
    "    scores = output['detection_scores']\n",
    "    num_detections = output['num_detections']\n",
    "    for i in range(int(num_detections[0])):\n",
    "        # if the confidence score is greater than the threshold we draw a bounding box around the object\n",
    "        if scores[0][i] > confidence_threshold:\n",
    "            box = boxes[0][i]\n",
    "            ymin = int(box[0] * image.shape[0])\n",
    "            xmin = int(box[1] * image.shape[1])\n",
    "            ymax = int(box[2] * image.shape[0])\n",
    "            xmax = int(box[3] * image.shape[1])\n",
    "            class_id = int(classes[0][i])\n",
    "            # get the name of the class from the dictionary, replace with 'Label unknown' if not found\n",
    "            display_name = id_to_display_name.get(class_id,f'Label unknown {class_id}')\n",
    "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)  # drawing rectangle\n",
    "            text = f\"{display_name} {scores[0][i]:.2f}\"  # text to be displayed along with confidence score\n",
    "            cv2.putText(image, text, (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    # plotting the image\n",
    "    fig, axs = plt.subplots(1, 2,figsize=(20, 20))\n",
    "    # ax[0] shows the output of the model and ax[1] shows the original image\n",
    "    # for ax[0] we convert the image from BGR to RGB since cv2 reads images in BGR format\n",
    "    axs[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    axs[1].imshow(image_stock)\n",
    "    axs[0].set_title('Inference Engine Output')\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('Original Image')\n",
    "    axs[0].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the label map, we will use urlopen method from [urllib.response](https://docs.python.org/3/library/urllib.request.html) library to work with [classes.json](https://raw.githubusercontent.com/RuneSkovrupHansen/openvino-examples/bb945203b32647dd37e0eff9a07564c33fe7b7b7/models/ssd_mobilenet_v2_coco/classes.json). An added advantage here is that we won't have to download and save the file itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "file_source = 'https://raw.githubusercontent.com/RuneSkovrupHansen/openvino-examples/bb945203b32647dd37e0eff9a07564c33fe7b7b7/models/ssd_mobilenet_v2_coco/classes.json'\n",
    "url_response = urlopen(file_source)  # open the url and record the response\n",
    "json_object = json.loads(url_response.read())  # read the response and convert it to a json object\n",
    "id_to_display_name = {int(k):v for k,v in json_object.items()}  # map json object to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "image_path = Path('../data/image/coco_bike.jpg')  # set image path\n",
    "processed_image = preprocess_image(str(image_path))  # preprocess the image\n",
    "output_original = model(processed_image)  # run the model on the image\n",
    "inference_engine(str(image_path),output_original,id_to_display_name,confidence_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Converting model to IR format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = Path('model')\n",
    "mo_command = f\"\"\"mo\n",
    "                 --saved_model_dir \"{model_dir/'saved_model'}\"\n",
    "                 --input_shape \"[1,320,320,3]\"\n",
    "                 --model_name \"{model_name}\"\n",
    "                 --tensorflow_object_detection_api_pipeline_config \"{model_dir/'pipeline.config'}\"\n",
    "                 --output_dir \"{output_dir/model_name}\"\n",
    "                 \"\"\"\n",
    "mo_command = \" \".join(mo_command.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "if not output_dir.exists():\n",
    "    print(\"Exporting TensorFlow model, please wait patiently!\")\n",
    "    ! $mo_command\n",
    "else:\n",
    "    print(f\"IR model {output_dir} already exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Observing results with IR model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# here we load the model into the OpenVINO runtime, read the model and compile it for the CPU\n",
    "core = ov.Core()  # starting the OpenVINO runtime\n",
    "model_ov = core.read_model(f'{str(output_dir/model_name/model_name)}.xml',f'{str(output_dir/model_name/model_name)}.bin')  # reading the model\n",
    "model_ov = core.compile_model(model_ov,\"CPU\")  # compiling on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# here we preprocess the image and convert it to an OpenVINO tensor\n",
    "image_preprocessed_ov = np.array(preprocess_image(str(image_path)))  # preprocessing the image converting to numpy array as OpenVINO expects it that way\n",
    "input_tensor = ov.Tensor(array=image_preprocessed_ov)  # setting the image as the input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# here we create an inference request and set the input tensor\n",
    "infer_request = model_ov.create_infer_request()  # creating an inference request\n",
    "infer_request.set_input_tensor(input_tensor)  # setting the input tensor to the inference request object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# here we start the inference and wait for it to finish\n",
    "infer_request.start_async()\n",
    "infer_request.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# here we create an empty dictionary and assign the output tensors to it, we can then use the dictionary to visualize the output\n",
    "output_ov = {}\n",
    "output_ov['num_detections'] = infer_request.get_output_tensor(7).data\n",
    "output_ov['detection_scores'] = infer_request.get_output_tensor(6).data\n",
    "output_ov['detection_boxes'] = infer_request.get_output_tensor(3).data\n",
    "output_ov['detection_classes'] = infer_request.get_output_tensor(4).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "inference_engine(str(image_path),output_ov,id_to_display_name,confidence_threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
