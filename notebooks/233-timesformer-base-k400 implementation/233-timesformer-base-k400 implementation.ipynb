{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert and Optimize TimeSformer with OpenVINOâ„¢\n",
    "TimeSformer(from Time-Space Transformer) devised and proposed in [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) uses self-attention layers and feedforward layers to extract features from frames. The main innovation here is the use of a modality called \"pose\" which refers to the spatial location and orientation of an object in video. By incorporating this information,TimeSformer is able to keep track of fine-grained details of activity of objects in the video. \n",
    "\n",
    "The variation of model that we will be using is the base model trained on kinetics400 dataset. The compiled model in available at [hugging face](https://huggingface.co/facebook/timesformer-base-finetuned-k400)\n",
    "\n",
    "The tutorial consists of the following steps:\n",
    "- Prepare PyTorch model\n",
    "- Download and prepare a dataset\n",
    "- Validate the original model\n",
    "- Convert PyTorch model to OpenVINO IR\n",
    "- Validate converted model\n",
    "- Prepare and run optimization pipeline\n",
    "- Compare performance of the FP32 and quantized models.\n",
    "- Compare accuracy of the FP32 and quantized models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa721c0b0227924e1313fb54ae915d2c7d3543290c470868f96f7f07b40b2667"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
